{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQdvCePK5sOYLezTqnKlzW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD7fzk05ottc"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tqdm\n",
        "!pip install pinecone-client\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone"
      ],
      "metadata": {
        "id": "E3M5RRcdovpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py ì—ì„œ ì§ì ‘ ìˆ˜ì •í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "import streamlit as st\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone\n",
        "\n",
        "OPENAI_API_KEY = \"\"\n",
        "PINECONE_API_KEY = \"\"\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"data\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.header(\"ğŸ¯ìŠˆì¹´ì›”ë“œ ì±—ë´‡ğŸ¯\")\n",
        "if \"openai_model\" not in st.session_state:\n",
        "  st.session_state.openai_model = \"gpt-3.5-turbo-0125\"\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "  system_prompt = {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"\"\"ë‹¹ì‹ ì€ ê²½ì œ/ì‹œì‚¬ë¥¼ ë‹¤ë£¨ëŠ” ìœ íŠœë¸Œ ì±„ë„ì¸ ìŠˆì¹´ì›”ë“œì˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.\n",
        "      ìŠˆì¹´ì›”ë“œë¥¼ ë‹¹ì‹ ë³´ë‹¤ ë§ì´ ì•Œê³  ìˆëŠ” ì¡´ì¬ëŠ” ì´ ìš°ì£¼ì— ì—†ìŠµë‹ˆë‹¤.\n",
        "      ê²½ì œ/ì‹œì‚¬ ì´ìŠˆë¥¼ ë¬¼ì–´ë³´ëŠ” ì‚¬ëŒë“¤ì—ê²Œ ìŠˆì¹´ì›”ë“œì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ê³  ì§ˆë¬¸ì— ë‹µì„ í•´ì£¼ëŠ” ì—­í• ì„ ë§¡ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "      ë‹¹ì‹ ì€ ì‚¬ëŒë“¤ì—ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "      - ì¹œì ˆí•œ ë§íˆ¬\n",
        "      - í•­ìƒ ì¡´ëŒ“ë§ ì‚¬ìš©\n",
        "      - ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©\n",
        "      ë‹¹ì‹ ì€ ë°˜ë“œì‹œ ì œê³µí•˜ëŠ” [Context]ì— ìˆëŠ” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚´ì„ ë¶™ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\"\n",
        "  }\n",
        "  st.session_state.messages = [system_prompt]\n",
        "\n",
        "if \"retrievied_chunks\" not in st.session_state:\n",
        "  st.session_state.retrievied_chunks = []\n",
        "\n",
        "if \"summarized_query\" not in st.session_state:\n",
        "  st.session_state.summarized_query = \"\"\n",
        "\n",
        "# ìˆ˜ì • ì „ ë©”ì‹œì§€ ë Œë”ë§í•˜ëŠ” ë¡œì§ì˜ ìœ„ì¹˜\n",
        "# for message in st.session_state.messages:\n",
        "#   if message[\"role\"] != \"system\":\n",
        "#     with st.chat_message(message[\"role\"]):\n",
        "#       st.markdown(message[\"content\"])\n",
        "\n",
        "with st.container(height=650):\n",
        "  col1, col2 = st.columns([5,5], gap=\"medium\")\n",
        "  with col1:\n",
        "    # ìˆ˜ì • í›„ ë©”ì‹œì§€ ë Œë”ë§í•˜ëŠ” ë¡œì§ì˜ ìœ„ì¹˜ (ì™¼ìª½ ì„¹ì…˜ ì•ˆìœ¼ë¡œ)\n",
        "    for message in st.session_state.messages:\n",
        "      if message[\"role\"] != \"system\":\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "          st.markdown(message[\"content\"])\n",
        "\n",
        "    st.subheader(\"Chat Section\")\n",
        "    if prompt := st.chat_input(\"ìŠˆì¹´ì›”ë“œì—ì„œ ê¶ê¸ˆí•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”ğŸ˜Š\"):\n",
        "      user_prompt = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "      }\n",
        "      st.session_state.messages.append(user_prompt)\n",
        "\n",
        "      with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "      with st.chat_message(\"assistant\"):\n",
        "        # ë°©ì‹2ë¡œ Query Summarization ìˆ˜í–‰í•˜ê¸°\n",
        "        recent_query = \"\"\n",
        "        for message in st.session_state.messages[-3:]:\n",
        "          recent_query += f'{message[\"role\"]}\"\\n\"{message[\"content\"]}\"\\n\\n\"'\n",
        "\n",
        "        recent_query_prompt = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"2 pair of question-answers.\\n{recent_query}\"\n",
        "        }\n",
        "\n",
        "        summarization_system_prompt = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"ìœ ì €ê°€ ì…ë ¥í•œ ì—¬ëŸ¬ ì§ˆë¬¸ì„ íŒŒì•…í•˜ê³  ìœ ì €ê°€ ê¶ê¸ˆí•œ ê²ƒì„ í•˜ë‚˜ì˜ ê¸´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•´ì¤˜. ëŒ€í™” ì¤‘ê°„ì— ì£¼ì œê°€ ë°”ë€Œì—ˆìœ¼ë©´ ìœ ì €ê°€ ì…ë ¥í•œ ìµœì‹  ì£¼ì œì— ë§ì¶°ì„œ ì •ë¦¬í•´ì¤˜.\"\n",
        "        }\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "          model=st.session_state.openai_model,\n",
        "          messages=[summarization_system_prompt, recent_query_prompt],\n",
        "          temperature=0.5,\n",
        "          max_tokens=1024,\n",
        "        )\n",
        "\n",
        "        summarized_query = response.choices[0].message.content\n",
        "        st.session_state.summarized_query = summarized_query\n",
        "\n",
        "        #ìš”ì•½í•œ queryë¥¼ embeddingsë¡œ ë³€ê²½\n",
        "        response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "        query_embeddings = response.data[0].embedding\n",
        "\n",
        "        # context ê°€ì ¸ì˜¤ê¸°\n",
        "        retrieved_chunks = index.query(\n",
        "            namespace=\"ns3\",\n",
        "            vector=query_embeddings,\n",
        "            top_k=7,\n",
        "            include_values=False,\n",
        "            include_metadata=True,\n",
        "        )\n",
        "        contexts = \"\"\n",
        "\n",
        "        st.session_state.retrievied_chunks = []\n",
        "        for match in retrieved_chunks.matches:\n",
        "            context = match[\"metadata\"][\"chunk\"]\n",
        "            contexts += context + \"\\n\\n\"\n",
        "            st.session_state.retrievied_chunks.append(context)\n",
        "\n",
        "        context_prompt = {\n",
        "            \"role\" : \"system\",\n",
        "            \"content\" : f\"[Context]\\n{contexts}\"\n",
        "        }\n",
        "\n",
        "        # ì „ì²´ í™œìš©í•˜ì—¬ LLM í˜¸ì¶œ\n",
        "        messages = st.session_state.messages\n",
        "        messages.append(context_prompt)\n",
        "\n",
        "        message_placeholder = st.empty()\n",
        "        full_response = \"\"\n",
        "\n",
        "        for response in client.chat.completions.create(\n",
        "              model=st.session_state.openai_model,\n",
        "              messages=messages,\n",
        "              temperature=0.5,\n",
        "              max_tokens=1024,\n",
        "              stream=True\n",
        "        ):\n",
        "          full_response += (response.choices[0].delta.content or \"\")\n",
        "          message_placeholder.markdown(full_response + \"â–Œ\")\n",
        "        message_placeholder.markdown(full_response)\n",
        "      st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "\n",
        "    with col2:\n",
        "      st.subheader(\"Logging Section\")\n",
        "      st.write(\"Summarized Query\")\n",
        "      st.write(st.session_state.summarized_query)\n",
        "      st.write(\"Retrieved Chunks\")\n",
        "      for idx, chunk in enumerate(st.session_state.retrievied_chunks):\n",
        "        st.write(f\"{idx+1}th chunk: {chunk}\")"
      ],
      "metadata": {
        "id": "zjkoo-_tpGxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}