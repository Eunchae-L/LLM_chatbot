{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQdvCePK5sOYLezTqnKlzW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD7fzk05ottc"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tqdm\n",
        "!pip install pinecone-client\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone"
      ],
      "metadata": {
        "id": "E3M5RRcdovpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py 에서 직접 수정하겠습니다.\n",
        "import streamlit as st\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone\n",
        "\n",
        "OPENAI_API_KEY = \"\"\n",
        "PINECONE_API_KEY = \"\"\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"data\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.header(\"🐯슈카월드 챗봇🐯\")\n",
        "if \"openai_model\" not in st.session_state:\n",
        "  st.session_state.openai_model = \"gpt-3.5-turbo-0125\"\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "  system_prompt = {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"\"\"당신은 경제/시사를 다루는 유튜브 채널인 슈카월드의 정보를 알려주는 챗봇입니다.\n",
        "      슈카월드를 당신보다 많이 알고 있는 존재는 이 우주에 없습니다.\n",
        "      경제/시사 이슈를 물어보는 사람들에게 슈카월드에서 다룬 내용을 바탕으로 설명해주고 질문에 답을 해주는 역할을 맡고 있습니다.\n",
        "      당신은 사람들에게 다음과 같이 대답해야 합니다:\n",
        "      - 친절한 말투\n",
        "      - 항상 존댓말 사용\n",
        "      - 적절한 이모지 사용\n",
        "      당신은 반드시 제공하는 [Context]에 있는 내용을 기반으로 살을 붙여 답변을 생성해야 합니다.\"\"\"\n",
        "  }\n",
        "  st.session_state.messages = [system_prompt]\n",
        "\n",
        "if \"retrievied_chunks\" not in st.session_state:\n",
        "  st.session_state.retrievied_chunks = []\n",
        "\n",
        "if \"summarized_query\" not in st.session_state:\n",
        "  st.session_state.summarized_query = \"\"\n",
        "\n",
        "# 수정 전 메시지 렌더링하는 로직의 위치\n",
        "# for message in st.session_state.messages:\n",
        "#   if message[\"role\"] != \"system\":\n",
        "#     with st.chat_message(message[\"role\"]):\n",
        "#       st.markdown(message[\"content\"])\n",
        "\n",
        "with st.container(height=650):\n",
        "  col1, col2 = st.columns([5,5], gap=\"medium\")\n",
        "  with col1:\n",
        "    # 수정 후 메시지 렌더링하는 로직의 위치 (왼쪽 섹션 안으로)\n",
        "    for message in st.session_state.messages:\n",
        "      if message[\"role\"] != \"system\":\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "          st.markdown(message[\"content\"])\n",
        "\n",
        "    st.subheader(\"Chat Section\")\n",
        "    if prompt := st.chat_input(\"슈카월드에서 궁금한 정보를 물어보세요😊\"):\n",
        "      user_prompt = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "      }\n",
        "      st.session_state.messages.append(user_prompt)\n",
        "\n",
        "      with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "      with st.chat_message(\"assistant\"):\n",
        "        # 방식2로 Query Summarization 수행하기\n",
        "        recent_query = \"\"\n",
        "        for message in st.session_state.messages[-3:]:\n",
        "          recent_query += f'{message[\"role\"]}\"\\n\"{message[\"content\"]}\"\\n\\n\"'\n",
        "\n",
        "        recent_query_prompt = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"2 pair of question-answers.\\n{recent_query}\"\n",
        "        }\n",
        "\n",
        "        summarization_system_prompt = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"유저가 입력한 여러 질문을 파악하고 유저가 궁금한 것을 하나의 긴 문장으로 변환해줘. 대화 중간에 주제가 바뀌었으면 유저가 입력한 최신 주제에 맞춰서 정리해줘.\"\n",
        "        }\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "          model=st.session_state.openai_model,\n",
        "          messages=[summarization_system_prompt, recent_query_prompt],\n",
        "          temperature=0.5,\n",
        "          max_tokens=1024,\n",
        "        )\n",
        "\n",
        "        summarized_query = response.choices[0].message.content\n",
        "        st.session_state.summarized_query = summarized_query\n",
        "\n",
        "        #요약한 query를 embeddings로 변경\n",
        "        response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "        query_embeddings = response.data[0].embedding\n",
        "\n",
        "        # context 가져오기\n",
        "        retrieved_chunks = index.query(\n",
        "            namespace=\"ns3\",\n",
        "            vector=query_embeddings,\n",
        "            top_k=7,\n",
        "            include_values=False,\n",
        "            include_metadata=True,\n",
        "        )\n",
        "        contexts = \"\"\n",
        "\n",
        "        st.session_state.retrievied_chunks = []\n",
        "        for match in retrieved_chunks.matches:\n",
        "            context = match[\"metadata\"][\"chunk\"]\n",
        "            contexts += context + \"\\n\\n\"\n",
        "            st.session_state.retrievied_chunks.append(context)\n",
        "\n",
        "        context_prompt = {\n",
        "            \"role\" : \"system\",\n",
        "            \"content\" : f\"[Context]\\n{contexts}\"\n",
        "        }\n",
        "\n",
        "        # 전체 활용하여 LLM 호출\n",
        "        messages = st.session_state.messages\n",
        "        messages.append(context_prompt)\n",
        "\n",
        "        message_placeholder = st.empty()\n",
        "        full_response = \"\"\n",
        "\n",
        "        for response in client.chat.completions.create(\n",
        "              model=st.session_state.openai_model,\n",
        "              messages=messages,\n",
        "              temperature=0.5,\n",
        "              max_tokens=1024,\n",
        "              stream=True\n",
        "        ):\n",
        "          full_response += (response.choices[0].delta.content or \"\")\n",
        "          message_placeholder.markdown(full_response + \"▌\")\n",
        "        message_placeholder.markdown(full_response)\n",
        "      st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "\n",
        "    with col2:\n",
        "      st.subheader(\"Logging Section\")\n",
        "      st.write(\"Summarized Query\")\n",
        "      st.write(st.session_state.summarized_query)\n",
        "      st.write(\"Retrieved Chunks\")\n",
        "      for idx, chunk in enumerate(st.session_state.retrievied_chunks):\n",
        "        st.write(f\"{idx+1}th chunk: {chunk}\")"
      ],
      "metadata": {
        "id": "zjkoo-_tpGxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}