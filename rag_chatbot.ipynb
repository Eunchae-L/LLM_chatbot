{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0tFfponnrQKJjztq7vB9n"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KGaGqLfm0hw"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "AhTpmcLEm8Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"your openai key\"\n",
        "PINECONE_API_KEY = \"your pinecone key\""
      ],
      "metadata": {
        "id": "ZPNM2Q3Am9Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken ___ìž…ë ¥í•˜ê¸°____"
      ],
      "metadata": {
        "id": "83cpi7mEm93M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "print(\"hello\")"
      ],
      "metadata": {
        "id": "8KJOPTBAm_rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone\n",
        "\n",
        "\n",
        "OPENAI_API_KEY = \"openai api key ë„£ê¸°\"\n",
        "PINECONE_API_KEY = \"pinecone api key ë„£ê¸°\"\n",
        "\n",
        "st.title(\"ðŸ¯ê³ ë ¤ëŒ€í•™êµ ì±—ë´‡ðŸ¯\")\n",
        "st.write(\"Hello â˜ï¸ðŸ¯KAIROS. ê³ ë ¤ëŒ€í•™êµì— ëŒ€í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"data\")\n",
        "\n",
        "if \"openai_model\" not in st.session_state:\n",
        "  st.session_state.openai_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "  st.session_state.messages = []\n",
        "\n",
        "system_prompt = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "      ë‹¹ì‹ ì€ 10ë…„ì°¨ ê²½ë ¥ì„ ê°€ì§„ ê³ ë ¤ëŒ€í•™êµ íˆ¬ì–´ ì±—ë´‡ìž…ë‹ˆë‹¤.\n",
        "      ëŒ€í•œë¯¼êµ­ì˜ ê³ ë ¤ëŒ€í•™êµë¥¼ ë‹¹ì‹ ë³´ë‹¤ ë§Žì´ ì•Œê³  ìžˆëŠ” ì¡´ìž¬ëŠ” ì´ ìš°ì£¼ì— ì—†ìŠµë‹ˆë‹¤.\n",
        "      ê³ ë ¤ëŒ€í•™êµì— íˆ¬ì–´ë¥¼ ì˜¨ ì‚¬ëžŒë“¤ì—ê²Œ ê³ ë ¤ëŒ€í•™êµì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ê³  ì§ˆë¬¸ì— ë‹µì„ í•´ì£¼ëŠ” ì—­í• ì„ ë§¡ê³  ìžˆìŠµë‹ˆë‹¤.\n",
        "      ì‚¬ëžŒë“¤ì€ ì£¼ë¡œ ì´ëŸ° ê²ƒë“¤ì„ ë¬¼ì–´ë´…ë‹ˆë‹¤:\n",
        "      - ê³ ë ¤ëŒ€í•™êµì˜ ì—­ì‚¬\n",
        "      - ê³ ë ¤ëŒ€í•™êµì˜ ê±´ë¬¼ë³„ íŠ¹ì§•\n",
        "      - ê³ ë ¤ëŒ€í•™êµì˜ ìž¬ë°ŒëŠ” ì‚¬ì‹¤ë“¤\n",
        "\n",
        "      ë‹¹ì‹ ì€ ì‚¬ëžŒë“¤ì—ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "      - ì¹œì ˆí•œ ë§íˆ¬\n",
        "      - í•­ìƒ ì¡´ëŒ“ë§ ì‚¬ìš©\n",
        "      - ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©\n",
        "\n",
        "      ë‹¹ì‹ ì€ ë°˜ë“œì‹œ ì œê³µí•˜ëŠ” [Context]ì— ìžˆëŠ” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚´ì„ ë¶™ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "  if message[\"role\"] != \"system\":\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "      st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input for chat\n",
        "if prompt := st.chat_input(\"ê³ ë ¤ëŒ€í•™êµì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”ðŸ˜Š\"):\n",
        "  # Add user message to chat history\n",
        "  st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "  # Display user message in chat message container\n",
        "  with st.chat_message(\"user\"):\n",
        "    st.markdown(prompt)\n",
        "\n",
        "\n",
        "  # Display assistant response in chat message container\n",
        "  with st.chat_message(\"assistant\"):\n",
        "    #ìž…ë ¥í•œ promptë¥¼ embedding\n",
        "    response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "    query_embeddings = response.data[0].embedding\n",
        "\n",
        "    # pineconeì˜ vector databaseì—ì„œ queryì™€ ê°€ê¹Œìš´ ë‹µë³€ ê°€ì ¸ì˜¤ê¸°\n",
        "    retrieved_chunks = index.query(\n",
        "        namespace=\"koreauniv\",\n",
        "        vector=query_embeddings,\n",
        "        top_k=5,\n",
        "        include_values=False,\n",
        "        include_metadata=True,\n",
        "    )\n",
        "    contexts = \"\"\n",
        "\n",
        "    for match in retrieved_chunks.matches:\n",
        "        contexts += match[\"metadata\"][\"chunk\"] + \"\\n\\n\"\n",
        "\n",
        "    context_prompt = {\n",
        "        \"role\" : \"system\",\n",
        "        \"content\" : f\"[Context]\\n{contexts}\"\n",
        "    }\n",
        "\n",
        "    messages = [system_prompt, context_prompt]\n",
        "    messages.extend(st.session_state.messages)\n",
        "\n",
        "    message_placeholder = st.empty()\n",
        "    full_response = \"\"\n",
        "\n",
        "    for response in client.chat.completions.create(\n",
        "          model=st.session_state.openai_model,\n",
        "          messages=messages,\n",
        "          temperature=0.5,\n",
        "          max_tokens=1024,\n",
        "          stream=True\n",
        "    ):\n",
        "      full_response += (response.choices[0].delta.content or \"\")\n",
        "      message_placeholder.markdown(full_response + \"â–Œ\")\n",
        "    message_placeholder.markdown(full_response)\n",
        "  st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})"
      ],
      "metadata": {
        "id": "kvG_kAwTnBj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!streamlit run app.py&>/dev/null&\n",
        "publ_url = ngrok.connect(addr='8501')\n",
        "publ_url"
      ],
      "metadata": {
        "id": "enM1ZwgMnC2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "id": "zjxEZ6QBnDVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill <streamlit ë²ˆí˜¸>"
      ],
      "metadata": {
        "id": "E7YzU1lfnD8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "EhTE41YGnEqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}