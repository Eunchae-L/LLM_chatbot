{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+3RvjpeeC6HWNy8fUC0KD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-Dkk9vjnOeT"
      },
      "outputs": [],
      "source": [
        "#langchain,tiktoken ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
        "!pip install langchain\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client\n",
        "!pip install tqdm\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "nlGp6WZ5nvpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"Î≥∏Ïù∏Ïùò openai api key\"\n",
        "PINECONE_API_KEY = \"Î≥∏Ïù∏Ïùò pinecone api key\""
      ],
      "metadata": {
        "id": "QFZa_W4nn1H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "MzXdkhLSn192"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken __"
      ],
      "metadata": {
        "id": "SQvfvephn2tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<Ï±óÎ¥áÏúºÎ°ú Rag with HyDEÏôÄ Rag without HyDEÎ•º ÎπÑÍµê.>**"
      ],
      "metadata": {
        "id": "Pa3vIgtKn4hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "print(\"hi\")"
      ],
      "metadata": {
        "id": "InkG6D-Ln66V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py ÏóêÏÑú ÏßÅÏ†ë ÏàòÏ†ïÌïòÍ≤†ÏäµÎãàÎã§.\n",
        "import streamlit as st\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone\n",
        "\n",
        "OPENAI_API_KEY = \"Î≥∏Ïù∏Ïùò openai api key\"\n",
        "PINECONE_API_KEY = \"Î≥∏Ïù∏Ïùò pinecone api key\"\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"data\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "st.header(\"üêØÏäàÏπ¥ÏõîÎìú Ï±óÎ¥áüêØ\")\n",
        "col1, col2 = st.columns([5,5])\n",
        "\n",
        "if \"openai_model\" not in st.session_state:\n",
        "  st.session_state.openai_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "  st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "  with st.chat_message(message[\"role\"]):\n",
        "    st.markdown(message[\"content\"])\n",
        "\n",
        "\n",
        "system_prompt = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"ÎãπÏã†ÏùÄ Í≤ΩÏ†ú/ÏãúÏÇ¨Î•º Îã§Î£®Îäî Ïú†ÌäúÎ∏å Ï±ÑÎÑêÏù∏ ÏäàÏπ¥ÏõîÎìúÏùò Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÎäî Ï±óÎ¥áÏûÖÎãàÎã§.\\\n",
        "        ÏäàÏπ¥ÏõîÎìúÎ•º ÎãπÏã†Î≥¥Îã§ ÎßéÏù¥ ÏïåÍ≥† ÏûàÎäî Ï°¥Ïû¨Îäî Ïù¥ Ïö∞Ï£ºÏóê ÏóÜÏäµÎãàÎã§.\\\n",
        "        Í≤ΩÏ†ú/ÏãúÏÇ¨ Ïù¥ÏäàÎ•º Î¨ºÏñ¥Î≥¥Îäî ÏÇ¨ÎûåÎì§ÏóêÍ≤å ÏäàÏπ¥ÏõîÎìúÏóêÏÑú Îã§Î£¨ ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú ÏÑ§Î™ÖÌï¥Ï£ºÍ≥† ÏßàÎ¨∏Ïóê ÎãµÏùÑ Ìï¥Ï£ºÎäî Ïó≠Ìï†ÏùÑ Îß°Í≥† ÏûàÏäµÎãàÎã§.\\\n",
        "        \\\n",
        "        ÎãπÏã†ÏùÄ ÏÇ¨ÎûåÎì§ÏóêÍ≤å Îã§ÏùåÍ≥º Í∞ôÏù¥ ÎåÄÎãµÌï¥Ïïº Ìï©ÎãàÎã§:\\\n",
        "        - ÏπúÏ†àÌïú ÎßêÌà¨\\\n",
        "        - Ìï≠ÏÉÅ Ï°¥ÎåìÎßê ÏÇ¨Ïö©\\\n",
        "        - Ï†ÅÏ†àÌïú Ïù¥Î™®ÏßÄ ÏÇ¨Ïö©\\\n",
        "        ÎãπÏã†ÏùÄ Î∞òÎìúÏãú Ï†úÍ≥µÌïòÎäî [Context]Ïóê ÏûàÎäî ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÇ¥ÏùÑ Î∂ôÏó¨ ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§.\"\n",
        "}\n",
        "\n",
        "if prompt := st.chat_input(\"Í∂ÅÍ∏àÌïú Ï†ïÎ≥¥Î•º Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöîüòä\"):\n",
        "  # Add user message to chat history\n",
        "  st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "  user_prompt = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": prompt\n",
        "  }\n",
        "\n",
        "  with col1:\n",
        "    st.write(\"Hello ‚òÅÔ∏è This is RAG without HyDE.\")\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      #ÏûÖÎ†•Ìïú promptÎ•º embedding\n",
        "      response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "      query_embeddings = response.data[0].embedding\n",
        "\n",
        "      # pineconeÏùò vector databaseÏóêÏÑú queryÏôÄ Í∞ÄÍπåÏö¥ ÎãµÎ≥Ä Í∞ÄÏ†∏Ïò§Í∏∞\n",
        "      retrieved_chunks = index.query(\n",
        "          namespace=\"ns1\",\n",
        "          vector=query_embeddings,\n",
        "          top_k=5,\n",
        "          include_values=False,\n",
        "          include_metadata=True,\n",
        "      )\n",
        "      contexts = \"\"\n",
        "\n",
        "      for match in retrieved_chunks.matches:\n",
        "          contexts += match[\"metadata\"][\"chunk\"] + \"\\n\\n\"\n",
        "\n",
        "      context_prompt = {\n",
        "          \"role\" : \"system\",\n",
        "          \"content\" : f\"[Context]\\n{contexts}\"\n",
        "      }\n",
        "\n",
        "      messages1 = [system_prompt, context_prompt, user_prompt]\n",
        "      messages1.extend(st.session_state.messages)\n",
        "\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "\n",
        "      for response in client.chat.completions.create(\n",
        "            model=st.session_state.openai_model,\n",
        "            messages=messages1,\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "            stream=True\n",
        "      ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "        message_placeholder.markdown(full_response + \"‚ñå\")\n",
        "      message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    with st.chat_message(\"system\"):\n",
        "      st.markdown(context_prompt)\n",
        "\n",
        "  with col2:\n",
        "    st.write(\"Hello ‚òÅÔ∏è This is RAG with HyDE.\")\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      hyde_prompts = [system_prompt, user_prompt]\n",
        "\n",
        "      #hypothetical ÎãµÎ≥ÄÏùÑ Î®ºÏ†Ä ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
        "      response = client.chat.completions.create(\n",
        "        model=st.session_state.openai_model,\n",
        "        messages=hyde_prompts,\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "      )\n",
        "      hyde_message = response.choices[0].message.content\n",
        "\n",
        "      # hallucinationÏù¥ Ìè¨Ìï®Îêú ÎãµÎ≥ÄÏùÑ embeddingsÎ°ú Íµ¨ÌïòÍ∏∞\n",
        "      response = client.embeddings.create(input=hyde_message, model=\"text-embedding-3-small\")\n",
        "      hyde_embeddings = response.data[0].embedding\n",
        "\n",
        "      st.markdown(hyde_message)\n",
        "\n",
        "      # pineconeÏùò vector databaseÏóêÏÑú hypothetical ÎãµÎ≥ÄÍ≥º Í∞ÄÏû• Í∞ÄÍπåÏö¥ Ï≤≠ÌÅ¨ Í∞ÄÏ†∏Ïò§Í∏∞\n",
        "      retrieved_chunks = index.query(\n",
        "        namespace=\"ns1\",\n",
        "        vector=hyde_embeddings,\n",
        "        top_k=7,\n",
        "        include_values=False,\n",
        "        include_metadata=True,\n",
        "      )\n",
        "      contexts = \"\"\n",
        "\n",
        "      for idx,match in enumerate(retrieved_chunks.matches):\n",
        "        contexts += match[\"metadata\"][\"chunk\"] + \"\\n\"\n",
        "\n",
        "      context_prompt = {\n",
        "        \"role\" : \"system\",\n",
        "        \"content\" : f\"[Context]\\n{contexts}\"\n",
        "      }\n",
        "\n",
        "      messages2 = [system_prompt, context_prompt, user_prompt]\n",
        "      messages2.extend(st.session_state.messages)\n",
        "\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "\n",
        "      for response in client.chat.completions.create(\n",
        "        model=st.session_state.openai_model,\n",
        "        messages=messages2,\n",
        "        temperature=0.5,\n",
        "        max_tokens=1500,\n",
        "        stream=True\n",
        "      ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "        message_placeholder.markdown(full_response + \"‚ñå\")\n",
        "      message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    with st.chat_message(\"system\"):\n",
        "      st.markdown(context_prompt)"
      ],
      "metadata": {
        "id": "Rq93IUZ-n_J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!streamlit run app.py&>/dev/null&\n",
        "publ_url = ngrok.connect(addr='8501')\n",
        "publ_url"
      ],
      "metadata": {
        "id": "iA4yWf7UoCqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "id": "Ba_gwiwCoDNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill"
      ],
      "metadata": {
        "id": "AbN25-vmoD1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÌååÏùº Í∏∞Î∞òÏúºÎ°ú title metadata Ï∂îÏ∂úÌïòÍ∏∞ (ns2 Íµ¨Ï∂ï)"
      ],
      "metadata": {
        "id": "IP3VYn8zoFM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "file_paths=[\n",
        "    \"/content/ÎÇ¥Í∞Ä Î∞òÎèÑÏ≤¥ ÏÇ∞ÏóÖÏùÑ Ïû¨Ìé∏ÌïòÍ≤†Îã§..txt\",\n",
        "    \"/content/ÎìúÎîîÏñ¥ Îì±Ïû•Ìïú Íµ≠ÎØºÏó∞Í∏à Í∞úÌòÅÏïà, Ïó¨Îü¨Î∂ÑÏùò ÏÑ†ÌÉùÏùÄ_.txt\",\n",
        "    \"/content/ÎØ∏Íµ≠ÏóêÏÑú Í∞ÄÏû• Îú®Í±∞Ïö¥ Ïª§ÎÆ§ÎãàÌã∞, Î†àÎîß.txt\",\n",
        "    \"/content/Î∞òÎèÖÏ†ê ÏÜåÏÜ°Ïóê ÌúòÎßêÎ¶∞ Ïï†Ìîå.txt\",\n",
        "    \"/content/ÏòÅÍµ≠Ïù∏Îì§ÏóêÍ≤å Ï∞®Î•º Í∏àÏßÄÌïòÎ©¥ ÏùºÏñ¥ÎÇòÎäî Ïùº.txt\",\n",
        "    \"/content/Ï°∞ÏûëÏùÄ ÌñàÏßÄÎßå, Ï£ºÍ∞ÄÎäî ÏµúÍ≥†Í∞Ä, ÎèÑÏöîÌÉÄ ÏáºÌÅ¨.txt\",\n",
        "    \"/content/ÏßÄÏßÄÏú® 1ÏúÑ, ÎèåÏïÑÏò® Ìä∏ÎüºÌîÑ.txt\",\n",
        "    \"/content/Ïπ¥Ïπ¥Ïò§ Ïó≠ÎåÄ ÏµúÍ≥†Í∞Ä Í≤ΩÏã†.txt\",\n",
        "    \"/content/ÌïúÍµ≠ Î∞òÎèÑÏ≤¥, Î¥ÑÏùÄ Ïò§ÎäîÍ∞Ä.txt\"\n",
        "]\n",
        "\n",
        "for file_path in tqdm(file_paths, desc=\"Processing file path...\"):\n",
        "  with open(file_path) as f:\n",
        "    text = f.read()\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "      chunk_size=512\n",
        "  )\n",
        "  texts = text_splitter.split_text(text)\n",
        "\n",
        "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "  vectors = []\n",
        "  title = file_path.split(\"/\")[2][:-3]\n",
        "\n",
        "  for chunk in tqdm(texts, desc=\"Processing chunks\"):\n",
        "      chunk_with_title = f\"[title]{title}\\n[script]{chunk}\"\n",
        "      response = client.embeddings.create(input=chunk_with_title, model=\"text-embedding-3-small\")\n",
        "      chunk_embeddings = response.data[0].embedding\n",
        "\n",
        "      chunk_id = str(uuid.uuid4())\n",
        "\n",
        "      vector_dict = {\n",
        "          \"id\": chunk_id,\n",
        "          \"values\": chunk_embeddings,\n",
        "          \"metadata\": {\"chunk\": chunk_with_title}\n",
        "      }\n",
        "      vectors.append(vector_dict)\n",
        "\n",
        "  batch_size = 10\n",
        "  num_batches = len(vectors) // 10 + 1\n",
        "\n",
        "  for i in tqdm(range(num_batches), desc=\"Processing chunks\"):\n",
        "      start_idx = i * batch_size\n",
        "      end_idx = start_idx + batch_size\n",
        "\n",
        "      batch_vectors = vectors[start_idx:end_idx]\n",
        "\n",
        "      index.upsert(vectors=batch_vectors, namespace=\"ns2\")"
      ],
      "metadata": {
        "id": "OoGYB26yoGE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py ÏóêÏÑú ÏßÅÏ†ë ÏàòÏ†ïÌïòÍ≤†ÏäµÎãàÎã§.\n",
        "import streamlit as st\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone\n",
        "\n",
        "OPENAI_API_KEY = \"Î≥∏Ïù∏Ïùò openai api key\"\n",
        "PINECONE_API_KEY = \"Î≥∏Ïù∏Ïùò pinecone api key\"\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"data\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "st.header(\"üêØÏäàÏπ¥ÏõîÎìú Ï±óÎ¥áüêØ\")\n",
        "col1, col2 = st.columns([5,5])\n",
        "\n",
        "if \"openai_model\" not in st.session_state:\n",
        "  st.session_state.openai_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "  st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "  with st.chat_message(message[\"role\"]):\n",
        "    st.markdown(message[\"content\"])\n",
        "\n",
        "\n",
        "system_prompt = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"ÎãπÏã†ÏùÄ Í≤ΩÏ†ú/ÏãúÏÇ¨Î•º Îã§Î£®Îäî Ïú†ÌäúÎ∏å Ï±ÑÎÑêÏù∏ ÏäàÏπ¥ÏõîÎìúÏùò Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÎäî Ï±óÎ¥áÏûÖÎãàÎã§.\\\n",
        "        ÏäàÏπ¥ÏõîÎìúÎ•º ÎãπÏã†Î≥¥Îã§ ÎßéÏù¥ ÏïåÍ≥† ÏûàÎäî Ï°¥Ïû¨Îäî Ïù¥ Ïö∞Ï£ºÏóê ÏóÜÏäµÎãàÎã§.\\\n",
        "        Í≤ΩÏ†ú/ÏãúÏÇ¨ Ïù¥ÏäàÎ•º Î¨ºÏñ¥Î≥¥Îäî ÏÇ¨ÎûåÎì§ÏóêÍ≤å ÏäàÏπ¥ÏõîÎìúÏóêÏÑú Îã§Î£¨ ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú ÏÑ§Î™ÖÌï¥Ï£ºÍ≥† ÏßàÎ¨∏Ïóê ÎãµÏùÑ Ìï¥Ï£ºÎäî Ïó≠Ìï†ÏùÑ Îß°Í≥† ÏûàÏäµÎãàÎã§.\\\n",
        "        \\\n",
        "        ÎãπÏã†ÏùÄ ÏÇ¨ÎûåÎì§ÏóêÍ≤å Îã§ÏùåÍ≥º Í∞ôÏù¥ ÎåÄÎãµÌï¥Ïïº Ìï©ÎãàÎã§:\\\n",
        "        - ÏπúÏ†àÌïú ÎßêÌà¨\\\n",
        "        - Ìï≠ÏÉÅ Ï°¥ÎåìÎßê ÏÇ¨Ïö©\\\n",
        "        - Ï†ÅÏ†àÌïú Ïù¥Î™®ÏßÄ ÏÇ¨Ïö©\\\n",
        "        ÎãπÏã†ÏùÄ Î∞òÎìúÏãú Ï†úÍ≥µÌïòÎäî [Context]Ïóê ÏûàÎäî ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÇ¥ÏùÑ Î∂ôÏó¨ ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§.\"\n",
        "}\n",
        "\n",
        "if prompt := st.chat_input(\"Í∂ÅÍ∏àÌïú Ï†ïÎ≥¥Î•º Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöîüòä\"):\n",
        "  # Add user message to chat history\n",
        "  st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "  user_prompt = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": prompt\n",
        "  }\n",
        "\n",
        "  with col1:\n",
        "    st.write(\"Hello ‚òÅÔ∏è This is RAG without title metadata\")\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      #ÏûÖÎ†•Ìïú promptÎ•º embedding\n",
        "      response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "      query_embeddings = response.data[0].embedding\n",
        "\n",
        "      # pineconeÏùò vector databaseÏóêÏÑú queryÏôÄ Í∞ÄÍπåÏö¥ ÎãµÎ≥Ä Í∞ÄÏ†∏Ïò§Í∏∞\n",
        "      retrieved_chunks = index.query(\n",
        "          namespace=\"ns1\",\n",
        "          vector=query_embeddings,\n",
        "          top_k=7,\n",
        "          include_values=False,\n",
        "          include_metadata=True,\n",
        "      )\n",
        "      contexts = \"\"\n",
        "\n",
        "      for match in retrieved_chunks.matches:\n",
        "          contexts += match[\"metadata\"][\"chunk\"] + \"\\n\\n\"\n",
        "\n",
        "      context_prompt = {\n",
        "          \"role\" : \"system\",\n",
        "          \"content\" : f\"[Context]\\n{contexts}\"\n",
        "      }\n",
        "\n",
        "      messages1 = [system_prompt, context_prompt, user_prompt]\n",
        "      messages1.extend(st.session_state.messages)\n",
        "\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "\n",
        "      for response in client.chat.completions.create(\n",
        "            model=st.session_state.openai_model,\n",
        "            messages=messages1,\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "            stream=True\n",
        "      ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "        message_placeholder.markdown(full_response + \"‚ñå\")\n",
        "      message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    with st.chat_message(\"system\"):\n",
        "      st.markdown(context_prompt)\n",
        "\n",
        "  with col2:\n",
        "    st.write(\"Hello ‚òÅÔ∏è This is RAG without title metadata\")\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      #ÏûÖÎ†•Ìïú promptÎ•º embedding\n",
        "      response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "      query_embeddings = response.data[0].embedding\n",
        "\n",
        "      # pineconeÏùò vector databaseÏóêÏÑú queryÏôÄ Í∞ÄÍπåÏö¥ ÎãµÎ≥Ä Í∞ÄÏ†∏Ïò§Í∏∞\n",
        "      retrieved_chunks = index.query(\n",
        "          namespace=\"ns2\",\n",
        "          vector=query_embeddings,\n",
        "          top_k=5,\n",
        "          include_values=False,\n",
        "          include_metadata=True,\n",
        "      )\n",
        "      contexts = \"\"\n",
        "\n",
        "      for match in retrieved_chunks.matches:\n",
        "          contexts += match[\"metadata\"][\"chunk\"] + \"\\n\\n\"\n",
        "\n",
        "      context_prompt = {\n",
        "          \"role\" : \"system\",\n",
        "          \"content\" : f\"[Context]\\n{contexts}\"\n",
        "      }\n",
        "\n",
        "      messages1 = [system_prompt, context_prompt, user_prompt]\n",
        "      messages1.extend(st.session_state.messages)\n",
        "\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "\n",
        "      for response in client.chat.completions.create(\n",
        "            model=st.session_state.openai_model,\n",
        "            messages=messages1,\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "            stream=True\n",
        "      ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "        message_placeholder.markdown(full_response + \"‚ñå\")\n",
        "      message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    with st.chat_message(\"system\"):\n",
        "      st.markdown(context_prompt)"
      ],
      "metadata": {
        "id": "JZZfhYCMoPZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!streamlit run app.py&>/dev/null&\n",
        "publ_url = ngrok.connect(addr='8501')\n",
        "publ_url"
      ],
      "metadata": {
        "id": "D7N8Q8nFoQYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "id": "wz3s9SMLoRNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill"
      ],
      "metadata": {
        "id": "wxP18Sp2oR3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}