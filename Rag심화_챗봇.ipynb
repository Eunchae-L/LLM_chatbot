{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+3RvjpeeC6HWNy8fUC0KD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-Dkk9vjnOeT"
      },
      "outputs": [],
      "source": [
        "#langchain,tiktoken 라이브러리 설치\n",
        "!pip install langchain\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client\n",
        "!pip install tqdm\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "nlGp6WZ5nvpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"본인의 openai api key\"\n",
        "PINECONE_API_KEY = \"본인의 pinecone api key\""
      ],
      "metadata": {
        "id": "QFZa_W4nn1H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "MzXdkhLSn192"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken __"
      ],
      "metadata": {
        "id": "SQvfvephn2tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<챗봇으로 Rag with HyDE와 Rag without HyDE를 비교.>**"
      ],
      "metadata": {
        "id": "Pa3vIgtKn4hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "print(\"hi\")"
      ],
      "metadata": {
        "id": "InkG6D-Ln66V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py 에서 직접 수정하겠습니다.\n",
        "import streamlit as st\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone\n",
        "\n",
        "OPENAI_API_KEY = \"본인의 openai api key\"\n",
        "PINECONE_API_KEY = \"본인의 pinecone api key\"\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"data\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "st.header(\"🐯슈카월드 챗봇🐯\")\n",
        "col1, col2 = st.columns([5,5])\n",
        "\n",
        "if \"openai_model\" not in st.session_state:\n",
        "  st.session_state.openai_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "  st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "  with st.chat_message(message[\"role\"]):\n",
        "    st.markdown(message[\"content\"])\n",
        "\n",
        "\n",
        "system_prompt = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"당신은 경제/시사를 다루는 유튜브 채널인 슈카월드의 정보를 알려주는 챗봇입니다.\\\n",
        "        슈카월드를 당신보다 많이 알고 있는 존재는 이 우주에 없습니다.\\\n",
        "        경제/시사 이슈를 물어보는 사람들에게 슈카월드에서 다룬 내용을 바탕으로 설명해주고 질문에 답을 해주는 역할을 맡고 있습니다.\\\n",
        "        \\\n",
        "        당신은 사람들에게 다음과 같이 대답해야 합니다:\\\n",
        "        - 친절한 말투\\\n",
        "        - 항상 존댓말 사용\\\n",
        "        - 적절한 이모지 사용\\\n",
        "        당신은 반드시 제공하는 [Context]에 있는 내용을 기반으로 살을 붙여 답변을 생성해야 합니다.\"\n",
        "}\n",
        "\n",
        "if prompt := st.chat_input(\"궁금한 정보를 물어보세요😊\"):\n",
        "  # Add user message to chat history\n",
        "  st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "  user_prompt = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": prompt\n",
        "  }\n",
        "\n",
        "  with col1:\n",
        "    st.write(\"Hello ☁️ This is RAG without HyDE.\")\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      #입력한 prompt를 embedding\n",
        "      response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "      query_embeddings = response.data[0].embedding\n",
        "\n",
        "      # pinecone의 vector database에서 query와 가까운 답변 가져오기\n",
        "      retrieved_chunks = index.query(\n",
        "          namespace=\"ns1\",\n",
        "          vector=query_embeddings,\n",
        "          top_k=5,\n",
        "          include_values=False,\n",
        "          include_metadata=True,\n",
        "      )\n",
        "      contexts = \"\"\n",
        "\n",
        "      for match in retrieved_chunks.matches:\n",
        "          contexts += match[\"metadata\"][\"chunk\"] + \"\\n\\n\"\n",
        "\n",
        "      context_prompt = {\n",
        "          \"role\" : \"system\",\n",
        "          \"content\" : f\"[Context]\\n{contexts}\"\n",
        "      }\n",
        "\n",
        "      messages1 = [system_prompt, context_prompt, user_prompt]\n",
        "      messages1.extend(st.session_state.messages)\n",
        "\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "\n",
        "      for response in client.chat.completions.create(\n",
        "            model=st.session_state.openai_model,\n",
        "            messages=messages1,\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "            stream=True\n",
        "      ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "        message_placeholder.markdown(full_response + \"▌\")\n",
        "      message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    with st.chat_message(\"system\"):\n",
        "      st.markdown(context_prompt)\n",
        "\n",
        "  with col2:\n",
        "    st.write(\"Hello ☁️ This is RAG with HyDE.\")\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      hyde_prompts = [system_prompt, user_prompt]\n",
        "\n",
        "      #hypothetical 답변을 먼저 생성합니다.\n",
        "      response = client.chat.completions.create(\n",
        "        model=st.session_state.openai_model,\n",
        "        messages=hyde_prompts,\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "      )\n",
        "      hyde_message = response.choices[0].message.content\n",
        "\n",
        "      # hallucination이 포함된 답변을 embeddings로 구하기\n",
        "      response = client.embeddings.create(input=hyde_message, model=\"text-embedding-3-small\")\n",
        "      hyde_embeddings = response.data[0].embedding\n",
        "\n",
        "      st.markdown(hyde_message)\n",
        "\n",
        "      # pinecone의 vector database에서 hypothetical 답변과 가장 가까운 청크 가져오기\n",
        "      retrieved_chunks = index.query(\n",
        "        namespace=\"ns1\",\n",
        "        vector=hyde_embeddings,\n",
        "        top_k=7,\n",
        "        include_values=False,\n",
        "        include_metadata=True,\n",
        "      )\n",
        "      contexts = \"\"\n",
        "\n",
        "      for idx,match in enumerate(retrieved_chunks.matches):\n",
        "        contexts += match[\"metadata\"][\"chunk\"] + \"\\n\"\n",
        "\n",
        "      context_prompt = {\n",
        "        \"role\" : \"system\",\n",
        "        \"content\" : f\"[Context]\\n{contexts}\"\n",
        "      }\n",
        "\n",
        "      messages2 = [system_prompt, context_prompt, user_prompt]\n",
        "      messages2.extend(st.session_state.messages)\n",
        "\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "\n",
        "      for response in client.chat.completions.create(\n",
        "        model=st.session_state.openai_model,\n",
        "        messages=messages2,\n",
        "        temperature=0.5,\n",
        "        max_tokens=1500,\n",
        "        stream=True\n",
        "      ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "        message_placeholder.markdown(full_response + \"▌\")\n",
        "      message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    with st.chat_message(\"system\"):\n",
        "      st.markdown(context_prompt)"
      ],
      "metadata": {
        "id": "Rq93IUZ-n_J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!streamlit run app.py&>/dev/null&\n",
        "publ_url = ngrok.connect(addr='8501')\n",
        "publ_url"
      ],
      "metadata": {
        "id": "iA4yWf7UoCqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "id": "Ba_gwiwCoDNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill"
      ],
      "metadata": {
        "id": "AbN25-vmoD1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "파일 기반으로 title metadata 추출하기 (ns2 구축)"
      ],
      "metadata": {
        "id": "IP3VYn8zoFM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "file_paths=[\n",
        "    \"/content/내가 반도체 산업을 재편하겠다..txt\",\n",
        "    \"/content/드디어 등장한 국민연금 개혁안, 여러분의 선택은_.txt\",\n",
        "    \"/content/미국에서 가장 뜨거운 커뮤니티, 레딧.txt\",\n",
        "    \"/content/반독점 소송에 휘말린 애플.txt\",\n",
        "    \"/content/영국인들에게 차를 금지하면 일어나는 일.txt\",\n",
        "    \"/content/조작은 했지만, 주가는 최고가, 도요타 쇼크.txt\",\n",
        "    \"/content/지지율 1위, 돌아온 트럼프.txt\",\n",
        "    \"/content/카카오 역대 최고가 경신.txt\",\n",
        "    \"/content/한국 반도체, 봄은 오는가.txt\"\n",
        "]\n",
        "\n",
        "for file_path in tqdm(file_paths, desc=\"Processing file path...\"):\n",
        "  with open(file_path) as f:\n",
        "    text = f.read()\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "      chunk_size=512\n",
        "  )\n",
        "  texts = text_splitter.split_text(text)\n",
        "\n",
        "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "  vectors = []\n",
        "  title = file_path.split(\"/\")[2][:-3]\n",
        "\n",
        "  for chunk in tqdm(texts, desc=\"Processing chunks\"):\n",
        "      chunk_with_title = f\"[title]{title}\\n[script]{chunk}\"\n",
        "      response = client.embeddings.create(input=chunk_with_title, model=\"text-embedding-3-small\")\n",
        "      chunk_embeddings = response.data[0].embedding\n",
        "\n",
        "      chunk_id = str(uuid.uuid4())\n",
        "\n",
        "      vector_dict = {\n",
        "          \"id\": chunk_id,\n",
        "          \"values\": chunk_embeddings,\n",
        "          \"metadata\": {\"chunk\": chunk_with_title}\n",
        "      }\n",
        "      vectors.append(vector_dict)\n",
        "\n",
        "  batch_size = 10\n",
        "  num_batches = len(vectors) // 10 + 1\n",
        "\n",
        "  for i in tqdm(range(num_batches), desc=\"Processing chunks\"):\n",
        "      start_idx = i * batch_size\n",
        "      end_idx = start_idx + batch_size\n",
        "\n",
        "      batch_vectors = vectors[start_idx:end_idx]\n",
        "\n",
        "      index.upsert(vectors=batch_vectors, namespace=\"ns2\")"
      ],
      "metadata": {
        "id": "OoGYB26yoGE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py 에서 직접 수정하겠습니다.\n",
        "import streamlit as st\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from pinecone import Pinecone\n",
        "\n",
        "OPENAI_API_KEY = \"본인의 openai api key\"\n",
        "PINECONE_API_KEY = \"본인의 pinecone api key\"\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"data\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "st.header(\"🐯슈카월드 챗봇🐯\")\n",
        "col1, col2 = st.columns([5,5])\n",
        "\n",
        "if \"openai_model\" not in st.session_state:\n",
        "  st.session_state.openai_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "  st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "  with st.chat_message(message[\"role\"]):\n",
        "    st.markdown(message[\"content\"])\n",
        "\n",
        "\n",
        "system_prompt = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"당신은 경제/시사를 다루는 유튜브 채널인 슈카월드의 정보를 알려주는 챗봇입니다.\\\n",
        "        슈카월드를 당신보다 많이 알고 있는 존재는 이 우주에 없습니다.\\\n",
        "        경제/시사 이슈를 물어보는 사람들에게 슈카월드에서 다룬 내용을 바탕으로 설명해주고 질문에 답을 해주는 역할을 맡고 있습니다.\\\n",
        "        \\\n",
        "        당신은 사람들에게 다음과 같이 대답해야 합니다:\\\n",
        "        - 친절한 말투\\\n",
        "        - 항상 존댓말 사용\\\n",
        "        - 적절한 이모지 사용\\\n",
        "        당신은 반드시 제공하는 [Context]에 있는 내용을 기반으로 살을 붙여 답변을 생성해야 합니다.\"\n",
        "}\n",
        "\n",
        "if prompt := st.chat_input(\"궁금한 정보를 물어보세요😊\"):\n",
        "  # Add user message to chat history\n",
        "  st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "  user_prompt = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": prompt\n",
        "  }\n",
        "\n",
        "  with col1:\n",
        "    st.write(\"Hello ☁️ This is RAG without title metadata\")\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      #입력한 prompt를 embedding\n",
        "      response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "      query_embeddings = response.data[0].embedding\n",
        "\n",
        "      # pinecone의 vector database에서 query와 가까운 답변 가져오기\n",
        "      retrieved_chunks = index.query(\n",
        "          namespace=\"ns1\",\n",
        "          vector=query_embeddings,\n",
        "          top_k=7,\n",
        "          include_values=False,\n",
        "          include_metadata=True,\n",
        "      )\n",
        "      contexts = \"\"\n",
        "\n",
        "      for match in retrieved_chunks.matches:\n",
        "          contexts += match[\"metadata\"][\"chunk\"] + \"\\n\\n\"\n",
        "\n",
        "      context_prompt = {\n",
        "          \"role\" : \"system\",\n",
        "          \"content\" : f\"[Context]\\n{contexts}\"\n",
        "      }\n",
        "\n",
        "      messages1 = [system_prompt, context_prompt, user_prompt]\n",
        "      messages1.extend(st.session_state.messages)\n",
        "\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "\n",
        "      for response in client.chat.completions.create(\n",
        "            model=st.session_state.openai_model,\n",
        "            messages=messages1,\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "            stream=True\n",
        "      ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "        message_placeholder.markdown(full_response + \"▌\")\n",
        "      message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    with st.chat_message(\"system\"):\n",
        "      st.markdown(context_prompt)\n",
        "\n",
        "  with col2:\n",
        "    st.write(\"Hello ☁️ This is RAG without title metadata\")\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      #입력한 prompt를 embedding\n",
        "      response = client.embeddings.create(input=prompt, model=\"text-embedding-3-small\")\n",
        "      query_embeddings = response.data[0].embedding\n",
        "\n",
        "      # pinecone의 vector database에서 query와 가까운 답변 가져오기\n",
        "      retrieved_chunks = index.query(\n",
        "          namespace=\"ns2\",\n",
        "          vector=query_embeddings,\n",
        "          top_k=5,\n",
        "          include_values=False,\n",
        "          include_metadata=True,\n",
        "      )\n",
        "      contexts = \"\"\n",
        "\n",
        "      for match in retrieved_chunks.matches:\n",
        "          contexts += match[\"metadata\"][\"chunk\"] + \"\\n\\n\"\n",
        "\n",
        "      context_prompt = {\n",
        "          \"role\" : \"system\",\n",
        "          \"content\" : f\"[Context]\\n{contexts}\"\n",
        "      }\n",
        "\n",
        "      messages1 = [system_prompt, context_prompt, user_prompt]\n",
        "      messages1.extend(st.session_state.messages)\n",
        "\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "\n",
        "      for response in client.chat.completions.create(\n",
        "            model=st.session_state.openai_model,\n",
        "            messages=messages1,\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "            stream=True\n",
        "      ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "        message_placeholder.markdown(full_response + \"▌\")\n",
        "      message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    with st.chat_message(\"system\"):\n",
        "      st.markdown(context_prompt)"
      ],
      "metadata": {
        "id": "JZZfhYCMoPZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!streamlit run app.py&>/dev/null&\n",
        "publ_url = ngrok.connect(addr='8501')\n",
        "publ_url"
      ],
      "metadata": {
        "id": "D7N8Q8nFoQYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "id": "wz3s9SMLoRNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill"
      ],
      "metadata": {
        "id": "wxP18Sp2oR3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}